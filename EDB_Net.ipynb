{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZkuSerFLm0b",
        "outputId": "11f8faf6-b2f6-4291-dec0-41f453fe422d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GuETSaBYVouL",
        "outputId": "a203933c-ba5e-42c1-cdc4-dbf899607b5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading papa-skin, 2947051697 bytes compressed\n",
            "[==================================================] 2947051697 bytes downloaded\n",
            "Downloaded and uncompressed: papa-skin\n",
            "Data source import complete.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'papa-skin:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4715525%2F8006502%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240504%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240504T160053Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D4b845e7237a41092580f041b4b724b779e7f13d4d9968b3d5956b2834bc541226d40bd946bab0833d940526029fa62167cf33a550526c82a2cfe18c0fad4d800bdf75be12a5234550352c241ec808cba83ebc511a64efd643f85ef0dd6618127727a4f1abb9a78b34d501deec32b0fcdc259d1633c144d9720cde805fb61253198a9e86f55b5760e0bc0dc3db0e016a3e580699862f3b08961afb8dc0893cbde52a9a7c1556ab5021fbda3e2c7e0c70ebfe78e41c5a2d79678315d8fb86f5fafd0dc59b8a93d1a3e04b14610135e2fb64f0de0fd079f433737c5bd69c3d40165eaf416ada7fe5e3a95965549df1e53fbdeca737c43b26b0d406cbcb98e532e3c'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CeKpPezgMEfx"
      },
      "outputs": [],
      "source": [
        "from keras import Model\n",
        "from keras.layers import Dense, Flatten, concatenate\n",
        "from keras.layers import Input, Conv2D, MaxPooling2D, BatchNormalization, ReLU, Concatenate, GlobalAveragePooling2D, Dense, Reshape, Activation\n",
        "from keras.applications import DenseNet201\n",
        "import tensorflow as tf\n",
        "from keras import layers, Model, Input\n",
        "from keras.layers import GlobalAveragePooling2D, Reshape, Conv2D, Multiply, SeparableConv2D, LayerNormalization, Dropout, BatchNormalization\n",
        "from keras.layers import Lambda, DepthwiseConv2D, BatchNormalization, Dropout, Reshape, UpSampling2D, Multiply\n",
        "from keras.layers import Conv2D, Multiply\n",
        "from keras import backend as K\n",
        "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from sklearn.metrics import  precision_score, recall_score, accuracy_score,classification_report ,confusion_matrix\n",
        "from tensorflow.python.platform import build_info as tf_build_info"
      ],
      "metadata": {
        "id": "flu31Ud7Z_eo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rysGK0uSMHaE"
      },
      "outputs": [],
      "source": [
        "class CustomDataGenerator:\n",
        "    def __init__(self, directory1, directory2, batch_size=16, img_height=224, img_width=224):\n",
        "        self.directory1 = directory1\n",
        "        self.directory2 = directory2\n",
        "        self.batch_size = batch_size\n",
        "        self.img_height = img_height\n",
        "        self.img_width = img_width\n",
        "        self.classes = sorted(os.listdir(directory1))\n",
        "        self.num_classes = len(self.classes)\n",
        "        self.class_to_index = {self.classes[i]: i for i in range(self.num_classes)}\n",
        "        self.index_to_class = {v: k for k, v in self.class_to_index.items()}\n",
        "\n",
        "\n",
        "        self.file_paths1 = []\n",
        "        self.file_paths2 = []\n",
        "        self.labels = []\n",
        "\n",
        "        for class_name in self.classes:\n",
        "            class_dir1 = os.path.join(directory1, class_name)\n",
        "            class_files1 = os.listdir(class_dir1)\n",
        "            self.file_paths1.extend([os.path.join(class_dir1, file) for file in class_files1])\n",
        "            class_dir2 = os.path.join(directory2, class_name)\n",
        "            class_files2 = os.listdir(class_dir2)\n",
        "            self.file_paths2.extend([os.path.join(class_dir2, file) for file in class_files2])\n",
        "            self.labels.extend([self.class_to_index[class_name]] * len(class_files1))\n",
        "\n",
        "        self.num_samples = len(self.file_paths1)\n",
        "\n",
        "    def parse_image(self, img_path1, img_path2, label):\n",
        "        img1 = tf.io.read_file(img_path1)\n",
        "        img1 = tf.image.decode_jpeg(img1, channels=3)\n",
        "        img1 = tf.image.resize(img1, [self.img_height, self.img_width])\n",
        "        img1 = tf.keras.applications.densenet.preprocess_input(img1)  # Or any other preprocessing you need\n",
        "        img2 = tf.io.read_file(img_path2)\n",
        "        img2 = tf.image.decode_jpeg(img2, channels=3)\n",
        "        img2 = tf.image.resize(img2, [self.img_height, self.img_width])\n",
        "        img2 = tf.keras.applications.densenet.preprocess_input(img2)\n",
        "        return (img1, img2), label\n",
        "\n",
        "    def create_dataset(self):\n",
        "        dataset = tf.data.Dataset.from_tensor_slices((self.file_paths1, self.file_paths2, self.labels))\n",
        "        dataset = dataset.shuffle(buffer_size=self.num_samples)\n",
        "        dataset = dataset.map(self.parse_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "        dataset = dataset.batch(self.batch_size)\n",
        "        dataset = dataset.repeat()\n",
        "        dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "        return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PgJlTRZIMLMq"
      },
      "outputs": [],
      "source": [
        "class CustomDataGenerator_test:\n",
        "    def __init__(self, directory1, directory2, batch_size=16, img_height=224, img_width=224):\n",
        "        self.directory1 = directory1\n",
        "        self.directory2 = directory2\n",
        "        self.batch_size = batch_size\n",
        "        self.img_height = img_height\n",
        "        self.img_width = img_width\n",
        "        self.classes = sorted(os.listdir(directory1))\n",
        "        self.num_classes = len(self.classes)\n",
        "        self.class_to_index = {self.classes[i]: i for i in range(self.num_classes)}\n",
        "        self.index_to_class = {v: k for k, v in self.class_to_index.items()}\n",
        "\n",
        "\n",
        "        self.file_paths1 = []\n",
        "        self.file_paths2 = []\n",
        "        self.labels = []\n",
        "\n",
        "        for class_name in self.classes:\n",
        "            class_dir1 = os.path.join(directory1, class_name)\n",
        "            class_files1 = os.listdir(class_dir1)\n",
        "            self.file_paths1.extend([os.path.join(class_dir1, file) for file in class_files1])\n",
        "            class_dir2 = os.path.join(directory2, class_name)\n",
        "            class_files2 = os.listdir(class_dir2)\n",
        "            self.file_paths2.extend([os.path.join(class_dir2, file) for file in class_files2])\n",
        "            self.labels.extend([self.class_to_index[class_name]] * len(class_files1))\n",
        "\n",
        "        self.num_samples = len(self.file_paths1)\n",
        "\n",
        "    def parse_image(self, img_path1, img_path2, label):\n",
        "        img1 = tf.io.read_file(img_path1)\n",
        "        img1 = tf.image.decode_jpeg(img1, channels=3)\n",
        "        img1 = tf.image.resize(img1, [self.img_height, self.img_width])\n",
        "        img1 = tf.keras.applications.densenet.preprocess_input(img1)  # Or any other preprocessing you need\n",
        "        img2 = tf.io.read_file(img_path2)\n",
        "        img2 = tf.image.decode_jpeg(img2, channels=3)\n",
        "        img2 = tf.image.resize(img2, [self.img_height, self.img_width])\n",
        "        img2 = tf.keras.applications.densenet.preprocess_input(img2)\n",
        "        return (img1, img2), label\n",
        "\n",
        "    def create_dataset(self):\n",
        "        dataset = tf.data.Dataset.from_tensor_slices((self.file_paths1, self.file_paths2, self.labels))\n",
        "        dataset = dataset.map(self.parse_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "        dataset = dataset.batch(self.batch_size)\n",
        "        dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "        return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLOBhUC2MTc1"
      },
      "outputs": [],
      "source": [
        "train_data1 = '/kaggle/input/papa-skin/Papa_skin/HED_edge_maps'\n",
        "train_data2 = '/kaggle/input/papa-skin/Papa_skin/train_dir'\n",
        "batch_size = 16\n",
        "img_height = 224\n",
        "img_width = 224\n",
        "\n",
        "train_data_generator = CustomDataGenerator(directory1=train_data1, directory2=train_data2, batch_size=batch_size, img_height=img_height, img_width=img_width)\n",
        "train_set = train_data_generator.create_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-U6h9MZMXgg"
      },
      "outputs": [],
      "source": [
        "test_data1 = '/kaggle/input/papa-skin/Papa_skin/HED_test'\n",
        "test_data2 = '/kaggle/input/papa-skin/Papa_skin/test_dir'\n",
        "batch_size = 16\n",
        "img_height = 224\n",
        "img_width = 224\n",
        "\n",
        "test_data_generator = CustomDataGenerator_test(directory1=test_data1, directory2=test_data2, batch_size=batch_size, img_height=img_height, img_width=img_width)\n",
        "test_set = test_data_generator.create_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tv87S8fwMeAR"
      },
      "outputs": [],
      "source": [
        "def dense_block(x, n_layers, growth_rate):\n",
        "    inputs = x\n",
        "    for _ in range(n_layers):\n",
        "        x = BatchNormalization()(x)\n",
        "        x = ReLU()(x)\n",
        "        x = Conv2D(growth_rate, (3, 3), strides=1, padding='same')(x)\n",
        "        inputs = Concatenate()([inputs, x])\n",
        "    return inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYWMHnYMMk8i"
      },
      "outputs": [],
      "source": [
        "def transition_block(x, n_filters):\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "    x = Conv2D(n_filters, (1, 1), strides=1, padding='same')(x)\n",
        "    x = MaxPooling2D((2, 2), strides=2)(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8JFI1JqMMlbd"
      },
      "outputs": [],
      "source": [
        "input_shape = (224, 224, 3)\n",
        "num_DT = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jdon2xt1MoL1"
      },
      "outputs": [],
      "source": [
        "# Branch 1\n",
        "\n",
        "# Input layer\n",
        "input1 = Input(shape=input_shape)\n",
        "\n",
        "# Initial convolution\n",
        "x = Conv2D(64, (3, 3), strides=2, padding='same')(input1)\n",
        "x = BatchNormalization()(x)\n",
        "x = ReLU()(x)\n",
        "\n",
        "# Dense blocks and transition blocks\n",
        "for _ in range(num_DT):\n",
        "    x = dense_block(x, 4, 12)\n",
        "    x = transition_block(x, 128)\n",
        "edge_features_E = x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7Fykrp0Mse1"
      },
      "outputs": [],
      "source": [
        "def cssa(input_tensor, num_heads=5):\n",
        "    # Split input into multiple heads\n",
        "    input_shape = input_tensor.shape\n",
        "    depth = input_shape[-1]\n",
        "    # Adjust depth to ensure it is evenly divisible by num_heads\n",
        "    new_depth = depth + (num_heads - (depth % num_heads))\n",
        "\n",
        "    # Modify input tensor depth if necessary\n",
        "    if new_depth != depth:\n",
        "        input_tensor = tf.keras.layers.Conv2D(new_depth, kernel_size=(1, 1))(input_tensor)\n",
        "\n",
        "    # Calculate depth per head\n",
        "    depth_per_head = new_depth // num_heads\n",
        "\n",
        "\n",
        "    # Channel attention\n",
        "    channel_avg = layers.GlobalAveragePooling2D()(input_tensor)\n",
        "    channel_max = layers.Lambda(lambda x: tf.reduce_max(x, axis=[1, 2]))(input_tensor)\n",
        "    channel_concat = layers.Concatenate(axis=1)([channel_avg, channel_max])\n",
        "\n",
        "    channel_dense1 = layers.Dense(units=depth // num_heads, activation='sigmoid')(channel_concat)\n",
        "    channel_attention = layers.Reshape((1, 1, depth // num_heads))(channel_dense1)\n",
        "\n",
        "    channel_attention = layers.UpSampling2D(size=(input_tensor.shape[1], input_tensor.shape[2]))(channel_attention)\n",
        "\n",
        "    # Spatial attention\n",
        "    spatial_avg = layers.Lambda(lambda x: tf.reduce_mean(x, axis=-1, keepdims=True))(input_tensor)\n",
        "    spatial_max = layers.Lambda(lambda x: tf.reduce_max(x, axis=-1, keepdims=True))(input_tensor)\n",
        "\n",
        "    spatial_concat = layers.Concatenate(axis=-1)([spatial_avg, spatial_max])\n",
        "    spatial_conv = layers.Conv2D(filters=num_heads, kernel_size=3, padding='same', activation='sigmoid')(spatial_concat)\n",
        "\n",
        "    spatial_attention = layers.Lambda(lambda x: tf.split(x, num_heads, axis=-1))(spatial_conv)\n",
        "\n",
        "\n",
        "\n",
        "    attention_map = layers.Multiply()([channel_attention] + spatial_attention)\n",
        "    attention_map = layers.Conv2D(filters=1, kernel_size=3, padding='same', activation='sigmoid')(attention_map)\n",
        "\n",
        "\n",
        "    # Apply attention to modulate the input features\n",
        "    feature_map = layers.Multiply()([input_tensor, attention_map])\n",
        "    feature_map = BatchNormalization()(feature_map)\n",
        "    feature_map = Dropout(0.02)(feature_map)\n",
        "\n",
        "\n",
        "    return feature_map, attention_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_1Dw97cMwtQ"
      },
      "outputs": [],
      "source": [
        "#Branch 2\n",
        "dropout_rate = 0.02\n",
        "l2_reg = 0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKhJZoHgMzkZ"
      },
      "outputs": [],
      "source": [
        "input2 = Input(shape=input_shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrvlbaPKM1q-",
        "outputId": "08f6db9d-25c6-403d-e3a1-dc1a0a3a07a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet201_weights_tf_dim_ordering_tf_kernels.h5\n",
            "82524592/82524592 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "densenet = tf.keras.applications.DenseNet201(\n",
        "    include_top=True,\n",
        "    weights=\"imagenet\",\n",
        "    input_tensor=input2,\n",
        "    input_shape=input_shape,\n",
        "    pooling=None,\n",
        "\n",
        ")\n",
        "# Exclude the last 28 layers of the model.\n",
        "dermo_features = densenet.layers[-28].output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDHtTg0PM42w"
      },
      "outputs": [],
      "source": [
        "dermo_features = BatchNormalization()(dermo_features)\n",
        "dermo_features = Dropout(0.02)(dermo_features)\n",
        "# attention block\n",
        "feature_maps, attention_map_A = cssa(dermo_features)\n",
        "feature_maps = BatchNormalization()(feature_maps)\n",
        "feature_maps = Dropout(0.02)(feature_maps)\n",
        "\n",
        "feature_maps = SeparableConv2D( 512, kernel_size=(3, 3), activation='sigmoid',\n",
        "            depthwise_initializer='he_normal', pointwise_initializer='he_normal',\n",
        "            depthwise_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
        "            pointwise_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
        "            padding='same')(feature_maps)\n",
        "feature_maps = BatchNormalization()(feature_maps)\n",
        "feature_maps = Dropout(0.02)(feature_maps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0rESMP0M-YO"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Layer\n",
        "\n",
        "class ResizeLayer(Layer):\n",
        "    def __init__(self, size, **kwargs):\n",
        "        super(ResizeLayer, self).__init__(**kwargs)\n",
        "        self.size = size\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return tf.image.resize(inputs, self.size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.constraints import MinMaxNorm\n",
        "\n",
        "class BetaLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, initializer=\"custom_initializer\", **kwargs):\n",
        "        super(BetaLayer, self).__init__(**kwargs)\n",
        "        self.beta_initializer = initializer\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.beta = self.add_weight(\n",
        "            name='beta',\n",
        "            shape=(1,),\n",
        "            initializer=self.beta_initializer,\n",
        "            trainable=True,\n",
        "            constraint=MinMaxNorm(min_value=0.0, max_value=1.0)  # Constraining beta to [0, 1]\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        multiplicative_input, additive_input = inputs\n",
        "        return self.beta * multiplicative_input + (1 - self.beta) * additive_input\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(BetaLayer, self).get_config()\n",
        "        config.update({\"initializer\": self.beta_initializer})\n",
        "        return config\n",
        "\n",
        "# Custom initializer to initialize beta to 0.5\n",
        "def custom_initializer(shape, dtype=None):\n",
        "    return tf.constant(0.5, shape=shape, dtype=dtype)"
      ],
      "metadata": {
        "id": "Ob39MUBe_DTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def enhanced_gating_mechanism(attention_maps, smoothed_edges, beta_initializer_value):\n",
        "    # Generate gating signal using a convolution followed by a sigmoid activation\n",
        "    gated_signal = layers.Conv2D(filters=smoothed_edges.shape[-1] // 2, kernel_size=(1, 1), padding='SAME', activation='sigmoid')(smoothed_edges)\n",
        "\n",
        "    # Learnable parameter for blending multiplicative and additive modulation\n",
        "    beta = tf.Variable(initial_value=tf.constant_initializer(value=beta_initializer_value)(shape=(1,)),\n",
        "                       trainable=True, dtype=tf.float32)\n",
        "\n",
        "    # Hybrid modulation\n",
        "    expanded_attention_maps = ResizeLayer(size=(28, 28))(attention_maps)\n",
        "    multiplicative_modulation = layers.Multiply()([gated_signal, expanded_attention_maps])\n",
        "    additive_modulation =  expanded_attention_maps\n",
        "\n",
        "    combined_attention = beta * multiplicative_modulation + (1 - beta) * additive_modulation\n",
        "\n",
        "    return combined_attention"
      ],
      "metadata": {
        "id": "Na9u2xUBULOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enhanced_attention_maps = enhanced_gating_mechanism(attention_map_A, edge_features_E)"
      ],
      "metadata": {
        "id": "M2S_ELZ3UniE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cosRHfYENEiu"
      },
      "outputs": [],
      "source": [
        "def aggregate_features_with_attention(feature_maps, modulated_attention_maps, dermo_features):\n",
        "\n",
        "    modulated_attention_maps = Conv2D(filters=128, kernel_size=(1, 1), padding='same')(modulated_attention_maps)\n",
        "    modulated_attention_maps = (MaxPooling2D(pool_size=(2, 2),padding=\"same\")(modulated_attention_maps))\n",
        "    modulated_attention_maps = (MaxPooling2D(pool_size=(2, 2),padding=\"same\")(modulated_attention_maps))\n",
        "    guided_feature_maps = Multiply()([dermo_features, modulated_attention_maps])\n",
        "    guided_feature_maps = Concatenate()([guided_feature_maps, feature_maps])\n",
        "    guided_feature_maps = BatchNormalization()(guided_feature_maps)\n",
        "    guided_feature_maps = Dropout(0.02)(guided_feature_maps)\n",
        "    return guided_feature_maps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YAZeKRaINIQG"
      },
      "outputs": [],
      "source": [
        "guided_features = aggregate_features_with_attention(feature_maps, enhanced_attention_maps, dermo_features)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "guided_features = Activation('relu')(guided_features)\n",
        "guided_features = Dropout(0.02)(guided_features)\n",
        "output = Flatten()(guided_features)\n",
        "output = Dense(7, activation='softmax')(output)"
      ],
      "metadata": {
        "id": "9RjIkddVXGVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFxBl2B_NP2H"
      },
      "outputs": [],
      "source": [
        "model = Model(inputs=[input1, input2], outputs=output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPGWY1OENTbi"
      },
      "outputs": [],
      "source": [
        "opt1=tf.keras.optimizers.Adam(learning_rate=0.001,epsilon=0.1)\n",
        "\n",
        "model.compile(optimizer=opt1,\n",
        "             loss='sparse_categorical_crossentropy',\n",
        "             metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6hzJrW7NVfS"
      },
      "outputs": [],
      "source": [
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, Callback\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath = '/content/drive/MyDrive/Papa/Final/EDB_Net.weights.h5',monitor='val_accuracy',\n",
        "                             save_best_only=True,save_weights_only=True)\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_accuracy', mode='min',patience=50, min_delta=0.001)\n",
        "\n",
        "reduce_learning_rate=ReduceLROnPlateau(monitor = \"val_accuracy\", patience=3, verbose=1, factor=0.5, min_lr=0.000001, cooldown=2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BetaLogger(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        beta_value = None\n",
        "        for layer in self.model.layers:\n",
        "            if isinstance(layer, BetaLayer):\n",
        "                beta_value = layer.beta.numpy()\n",
        "                break\n",
        "        print(f\"Epoch {epoch}: beta = {beta_value}\")"
      ],
      "metadata": {
        "id": "nAp5BnsnV5Ou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_set,\n",
        "                    steps_per_epoch = total_train_samples // batch_size,\n",
        "                    epochs=60,\n",
        "                    verbose=1,\n",
        "                    validation_data=test_set,\n",
        "                    validation_steps = total_valid_samples // batch_size,\n",
        "                    callbacks=[checkpoint, reduce_learning_rate, early_stop, BetaLogger()])"
      ],
      "metadata": {
        "id": "JgKJGw3hYxzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qraTS-fnjZuI"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import models\n",
        "model.load_weights('/content/drive/MyDrive/Papa/Final/EDB_Net.weights.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKxyi-wiQLny"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# Get true labels from the test dataset\n",
        "y_true = np.concatenate([label.numpy() for _, label in test_set])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(test_set)"
      ],
      "metadata": {
        "id": "OBV-MBdgbXia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzBze-YzjQ4L"
      },
      "outputs": [],
      "source": [
        "y_pred = np.argmax(predictions, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "targetnames = ['akiec', 'bcc', 'bkl', 'df', 'mel', 'nv', 'vasc']\n",
        "\n",
        "report = classification_report(y_true, y_pred, target_names=targetnames)\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(report)"
      ],
      "metadata": {
        "id": "OkFINC8WZI1t"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}